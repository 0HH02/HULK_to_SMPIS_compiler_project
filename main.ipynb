{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from hulk_compiler.lexer.token import Token\n",
    "from hulk_compiler.parser.ast.ast import ASTNode\n",
    "from hulk_compiler.parser.grammar.hulk import get_hulk_grammar\n",
    "from hulk_compiler.parser.parser_lr1 import ParserLR1\n",
    "from hulk_compiler.lexer.lexer import Lexer\n",
    "from hulk_compiler.lexer.hulk_token_patterns import TOKEN_PATTERNS\n",
    "from hulk_compiler.parser.ast.ast_printer import ASTPrinter\n",
    "\n",
    "lexer = Lexer(TOKEN_PATTERNS)\n",
    "\n",
    "grammar, mapping = get_hulk_grammar()\n",
    "\n",
    "parser = ParserLR1(grammar, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " program: {\n",
      "    type: {\n",
      "       identifier:  Token( Lex:Point, Type:TokenType.IDENTIFIER)\n",
      "       atribute: {\n",
      "          identifier:  Token( Lex:x, Type:TokenType.IDENTIFIER)\n",
      "          literal:  0\n",
      "       }\n",
      "       atribute: {\n",
      "          identifier:  Token( Lex:y, Type:TokenType.IDENTIFIER)\n",
      "          literal:  0\n",
      "       }\n",
      "       function_declaration: {\n",
      "          identifier:  Token( Lex:getX, Type:TokenType.IDENTIFIER)\n",
      "          attribute_call: {\n",
      "             identifier: Token( Lex:self, Type:TokenType.IDENTIFIER)\n",
      "             identifier:  Token( Lex:x, Type:TokenType.IDENTIFIER)\n",
      "          }\n",
      "       }\n",
      "       function_declaration: {\n",
      "          identifier:  Token( Lex:getY, Type:TokenType.IDENTIFIER)\n",
      "          attribute_call: {\n",
      "             identifier: Token( Lex:self, Type:TokenType.IDENTIFIER)\n",
      "             identifier:  Token( Lex:y, Type:TokenType.IDENTIFIER)\n",
      "          }\n",
      "       }\n",
      "       function_declaration: {\n",
      "          identifier:  Token( Lex:setX, Type:TokenType.IDENTIFIER)\n",
      "          parameter: {\n",
      "             identifier Token( Lex:x, Type:TokenType.IDENTIFIER)\n",
      "          }\n",
      "          destructive_assigment: {\n",
      "             identifier AttributeCall(obj=Identifier(identifier=Token( Lex:self, Type:TokenType.IDENTIFIER)), identifier=Token( Lex:x, Type:TokenType.IDENTIFIER))\n",
      "             identifier: Token( Lex:x, Type:TokenType.IDENTIFIER)\n",
      "          }\n",
      "       }\n",
      "       function_declaration: {\n",
      "          identifier:  Token( Lex:setY, Type:TokenType.IDENTIFIER)\n",
      "          parameter: {\n",
      "             identifier Token( Lex:y, Type:TokenType.IDENTIFIER)\n",
      "          }\n",
      "          destructive_assigment: {\n",
      "             identifier AttributeCall(obj=Identifier(identifier=Token( Lex:self, Type:TokenType.IDENTIFIER)), identifier=Token( Lex:y, Type:TokenType.IDENTIFIER))\n",
      "             identifier: Token( Lex:y, Type:TokenType.IDENTIFIER)\n",
      "          }\n",
      "       }\n",
      "    }\n",
      "    let: {\n",
      "       variable_declaration: {\n",
      "          identifier Token( Lex:pt, Type:TokenType.IDENTIFIER)\n",
      "          instanciate: {\n",
      "             identifier:  Token( Lex:Point, Type:TokenType.IDENTIFIER)\n",
      "          }\n",
      "       }\n",
      "       invocation: {\n",
      "          identifier:  Token( Lex:print, Type:TokenType.IDENTIFIER)\n",
      "          binary_node: {\n",
      "             operator:  Operator.CONCAT\n",
      "             binary_node: {\n",
      "                operator:  Operator.CONCAT\n",
      "                binary_node: {\n",
      "                   operator:  Operator.CONCAT\n",
      "                   literal:  \"x: \"\n",
      "                   function_call: {\n",
      "                      identifier: Token( Lex:pt, Type:TokenType.IDENTIFIER)\n",
      "as\n",
      "                      invocation: {\n",
      "                         identifier:  Token( Lex:getX, Type:TokenType.IDENTIFIER)\n",
      "                      }\n",
      "                   }\n",
      "                }\n",
      "                literal:  \"; y: \"\n",
      "             }\n",
      "             function_call: {\n",
      "                identifier: Token( Lex:pt, Type:TokenType.IDENTIFIER)\n",
      "as\n",
      "                invocation: {\n",
      "                   identifier:  Token( Lex:setY, Type:TokenType.IDENTIFIER)\n",
      "                   literal:  4\n",
      "                }\n",
      "             }\n",
      "          }\n",
      "       }\n",
      "    }\n",
      " }\n",
      "Execution time: 0.001222372055053711 seconds\n"
     ]
    }
   ],
   "source": [
    "PROGRAM = \"\"\"\n",
    "type Point() {\n",
    "    x = 0;\n",
    "    y = 0;\n",
    "\n",
    "    getX() => self.x;\n",
    "    getY() => self.y;\n",
    "\n",
    "    setX(x: Number) => self.x := x;\n",
    "    setY(y: Number) => self.y := y;\n",
    "}\n",
    "\n",
    "let pt = new Point() in\n",
    "    print(\"x: \" @ pt.getX() @ \"; y: \" @ pt.setY(4));\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "tokens: list[Token] = lexer.tokenize(PROGRAM)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "ast: ASTNode = parser.parse(tokens)\n",
    "ASTPrinter.visit_node(ast)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
